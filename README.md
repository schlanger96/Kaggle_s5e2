# 가방 가격 예측 경연 (Backpack Prediction Challenge)  

----------------------------------------------------

# 경연 개요  
본 경연은 가방 가격 예측 문제를 해결하기 위한 머신러닝 모델을 만드는 것이 목표

---------------------------------------------------

# 요약  
주어진 데이터는 일반적인 전처리와 하이퍼 파라미터 튜닝보다  
하나의 피처로 타겟인코딩한 RMSE가 더 낮을 정도로 타겟에 대한 신호가 미약한 데이터였다  
다양한 전처리 기법, 하이퍼파라미터 튜닝, 타겟 인코딩 실험을 수행하며 비교했다  
타겟인코딩 + 앙상블 모델(XGBoost + LGBM + CatBoost + Ridge)이 최종 성능 최고 기록  
총 20회 Kaggle 제출 → 최종 RMSE 38.90098 달성  
최종 등수 = 456 / 3393 (상위 13.4%)

---------------------------------------------------

# 1. 데이터 크기  
학습 데이터: 3,994,318행 × 10열  
검증 데이터: 200,000행 × 9열

--------------------------------------------------

# 2. 평가지표  
RMSE (Root Mean Squared Error, 평균 제곱근 오차)  

-------------------------------------------------

# 3. 변수 정보  

설명 변수 (Features)  
브랜드(Brand)  
소재(Material)  
크기(Size)  
수납공간 개수(Compartments)  
노트북 전용 수납공간(Laptop Compartment)  
방수 여부(Waterproof)  
스타일(Style)  
색상(Color)  
최대 하중(kg)(Weight Capacity (kg))  
가방 가격(Price)  

-------------------------------------------------

# 4. 시도해본 데이터 전처리

결측치 처리  
범주형 변수 → 최빈값 대체  
연속형 변수 → 중앙값 대체  
범주형 변수 결측치를 'Missing' 텍스트로 대체  
각 행별 결측치 개수를 새로운 변수로 추가  

특성 변환 및 인코딩  
범주형 변수 → 원-핫 인코딩(One-Hot Encoding)  
가장 높은 변수 중요도를 가진 Weight Capacity 변수를 타겟 인코딩(Target Encoding)  
변수 중요도 상위 3개의 변수만 타겟 인코딩  
모든 변수를 타겟 인코딩  
타겟 변수(Price)를 로그 변환  

모델 학습 및 튜닝  
하이퍼파라미터 탐색 공간을 확장하여 장기간 학습  
주요 파라미터를 먼저 튜닝한 후, 나머지 파라미터를 조정  

모델 앙상블 (Ensemble)  
(XGBoost, CatBoost, LGBM) 메타모델을 XGBoost로 앙상블  
(XGBoost, CatBoost, LGBM) 메타모델을 Ridge Regression으로 앙상블  

-------------------------------------------------------------

# 5. 최종 캐글 제출 모델 및 성능  

1) 앙상블 모델  
Kaggle Public Score: 38.90098  
모든 변수를 25분할 교차검증(Target Encoding) 적용  
XGBoost, LGBM, CatBoost를 사용하여 OOF 데이터 프레임 생성   
메타모델로 Ridge Regression 사용

2) XGBoost
Kaggle Public Score: 39.09524  
범주형 변수 결측치 → 'Missing' 텍스트 처리  
연속형 변수 결측치 → 중앙값 처리  
각 행별 결측치 개수를 새로운 변수로 추가

-----------------------------------------------------------------

# 6. 학습 과정에서 얻은 인사이트  

1. 데이터의 설명변수가 타겟변수를 예측할 수 있는 신호가 미약한 데이터일 때,  
일반적인 인코딩과 데이터 전처리 보다 모든 변수를 타겟 인코딩 하는 것이 성능이 좋을 수도 있다.  

사용한 주요 라이브러리  
Python, XGBoost, LGBM, CatBoost, Scikit-Learn, Pandas, NumPy, Optuna, TargetEncoder  

-----------------------------------------------------------------

# 7. 관련 링크
Kaggle 경진대회 페이지: [https://www.kaggle.com/competitions/playground-series-s5e2/overview]  
